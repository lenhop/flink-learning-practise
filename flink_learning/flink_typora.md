
## 1.FlinkåŸºç¡€æ¦‚å¿µ

Apache Flink æ˜¯ä¸€ä¸ª **åˆ†å¸ƒå¼æµå¤„ç†æ¡†æ¶**ï¼Œä¸“ä¸º **æœ‰çŠ¶æ€è®¡ç®—çš„æµæ•°æ®** è®¾è®¡ï¼ŒåŒæ—¶æ”¯æŒæ‰¹å¤„ç†ï¼ˆå°†æ‰¹æ•°æ®è§†ä¸ºæœ‰é™çš„æµï¼‰ã€‚

### 1.1è§£å†³ä¼ ç»Ÿè®¡ç®—æ¶æ„çš„ç—›ç‚¹

- å®æ—¶æ€§ç“¶é¢ˆ: ä¼ ç»Ÿæ‰¹å¤„ç†ï¼ˆå¦‚Hadoopï¼‰å»¶è¿Ÿè¾¾å°æ—¶çº§ï¼Œæ— æ³•æ»¡è¶³å®æ—¶ç›‘æ§ã€é£æ§ç­‰åœºæ™¯
- æµæ‰¹åˆ†ç¦»æˆæœ¬é«˜: Lambdaæ¶æ„éœ€ç»´æŠ¤ä¸¤å¥—ä»£ç ï¼ˆå®æ—¶+ç¦»çº¿ï¼‰ï¼Œå¼€å‘è¿ç»´å¤æ‚
- çŠ¶æ€ç®¡ç†è–„å¼±: ä¼ ç»Ÿæµå¤„ç†ï¼ˆå¦‚Stormï¼‰éš¾ä»¥ä¿è¯ç²¾ç¡®ä¸€æ¬¡ï¼ˆExactly-Onceï¼‰è¯­ä¹‰

###  1.2æ ¸å¿ƒåŠŸèƒ½ä¸ä½œç”¨

| **åŠŸèƒ½æ¨¡å—**     | **è§£å†³çš„é—®é¢˜**           | **å…¸å‹åœºæ™¯**           |
| ---------------- | ------------------------ | ---------------------- |
| **ä½å»¶è¿Ÿæµå¤„ç†** | æ¯«ç§’çº§å“åº”å®æ—¶æ•°æ®       | æ¬ºè¯ˆæ£€æµ‹ã€å®æ—¶å‘Šè­¦     |
| **æµæ‰¹ä¸€ä½“å¼•æ“** | åŒä¸€å¥—APIå¤„ç†æµ/æ‰¹æ•°æ®   | T+0å®æ—¶æŠ¥è¡¨ã€æ•°æ®è¡¥å…¨  |
| **ç²¾ç¡®çŠ¶æ€ç®¡ç†** | æ•…éšœæ¢å¤åæ•°æ®ä¸ä¸¢ä¸é‡   | é‡‘èäº¤æ˜“å¯¹è´¦ã€ç²¾å‡†è®¡è´¹ |
| **äº‹ä»¶æ—¶é—´å¤„ç†** | å¤„ç†ä¹±åºæ•°æ®ä¿è¯ç»“æœå‡†ç¡® | ç‰©è”ç½‘è®¾å¤‡æ•°æ®åˆ†æ     |

### 1.3å…³é”®åº”ç”¨åœºæ™¯

- å®æ—¶æ•°æ®ç®¡é“ï¼ˆETLï¼‰ï¼šå°† Kafka æ•°æ®å®æ—¶æ¸…æ´—åå†™å…¥æ•°ä»“ï¼ˆå¦‚ HBase/ClickHouseï¼‰ï¼Œ åˆ†é’Ÿçº§æ•°æ®å¯è§æ€§
- å®æ—¶ç›‘æ§ä¸å‘Šè­¦ï¼šæ£€æµ‹æœåŠ¡å™¨æŒ‡æ ‡ï¼ˆå¦‚ CPU > 90% æŒç»­ 1åˆ†é’Ÿï¼‰
- é‡‘èé£æ§ï¼šæ¯«ç§’çº§è¯†åˆ«å¼‚å¸¸äº¤æ˜“æ¨¡å¼ï¼ˆå¦‚çŸ­æ—¶é—´å†…å¤šç¬”å¤§é¢è½¬è´¦ï¼‰
- å®æ—¶æ¨èç³»ç»Ÿï¼šæ ¹æ®ç”¨æˆ·å®æ—¶è¡Œä¸ºï¼ˆç‚¹å‡»/è´­ä¹°ï¼‰æ›´æ–°æ¨èç»“æœ

æ€»ç»“ï¼šå®ç°éœ€è¦ **ä½å»¶è¿Ÿã€å¼ºä¸€è‡´æ€§ã€å¤æ‚çŠ¶æ€ç®¡ç†** çš„å®æ—¶è®¡ç®—åœºæ™¯

### 1.4Flinkæœ‰çŠ¶æ€è®¡ç®—æœ¬è´¨

**â€œæœ‰çŠ¶æ€â€ä¸ä»…ä¾èµ–å†å²æ•°æ®ï¼Œè¿˜æ¶µç›–æ‰€æœ‰éœ€è¦è·¨æ•°æ®è®°å½•ï¼ˆæˆ–è·¨æ—¶é—´ï¼‰ç»´æŠ¤ä¸­é—´ä¿¡æ¯çš„åœºæ™¯**ã€‚å…¶æœ¬è´¨æ˜¯ **ç®—å­ï¼ˆOperatorï¼‰åœ¨å¤„ç†æ•°æ®æ—¶éœ€è¦è®°ä½æŸäº›ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½æ¥è‡ªå†å²æ•°æ®ï¼Œä¹Ÿå¯èƒ½æ˜¯ç¨‹åºè¿è¡Œä¸­çš„ä¸´æ—¶çŠ¶æ€ã€‚

| **çŠ¶æ€ç±»å‹**       | **è¯´æ˜**                                             | **ç¤ºä¾‹**                             |
| ------------------ | ---------------------------------------------------- | ------------------------------------ |
| **æ˜¾å¼å†å²æ•°æ®**   | ç›´æ¥ä¾èµ–ä¹‹å‰å¤„ç†è¿‡çš„æ•°æ®è®¡ç®—ç»“æœã€‚                   | ç”¨æˆ·ç‚¹å‡»é‡ç´¯åŠ ã€æ»‘åŠ¨çª—å£å¹³å‡å€¼ã€‚     |
| **éšå¼ä¸Šä¸‹æ–‡ä¿¡æ¯** | ä¾èµ–ç¨‹åºè¿è¡Œä¸­ç”Ÿæˆçš„ä¸­é—´çŠ¶æ€ï¼ˆå¯èƒ½ä¸å†å²æ•°æ®æ— å…³ï¼‰ã€‚ | Kafka æ¶ˆè´¹åç§»é‡ã€æœºå™¨å­¦ä¹ æ¨¡å‹å‚æ•°ã€‚ |
| **å¤–éƒ¨ç³»ç»Ÿäº¤äº’**   | é€šè¿‡çŠ¶æ€ç¼“å­˜å¤–éƒ¨æ•°æ®ï¼Œå‡å°‘é‡å¤è®¿é—®ï¼ˆå¦‚ç»´è¡¨å…³è”ï¼‰ã€‚   | ç”¨æˆ·ç”»åƒç¼“å­˜ã€å•†å“ä»·æ ¼å¿«ç…§ã€‚         |



## 2.Flinkæ¶æ„ä¸ç»„ä»¶

### 2.1Flinkæ¶æ„ä¸ç»„ä»¶åŠŸèƒ½ 

flinkæ ¸å¿ƒç»„ä»¶åªæœ‰ JobManager å’Œ TaskManagerï¼Œä¸ºäº†æ–¹ä¾¿ç†è§£æ•´ä¸ªä½œä¸šæµç¨‹ï¼Œä¸‹é¢ç»™å‡ºæ•´ä¸ªä½œä¸šç»„ä»¶

**1. å®¢æˆ·ç«¯ï¼ˆClientï¼‰**

| ç»„ä»¶             | åŠŸèƒ½èŒè´£                                                     | å…³é”®ç‰¹æ€§                        |
| ---------------- | ------------------------------------------------------------ | ------------------------------- |
| **CLIå‘½ä»¤è¡Œ**    | é€šè¿‡å‘½ä»¤è¡Œæäº¤ä½œä¸šï¼ˆ`flink run`ï¼‰ã€å–æ¶ˆä½œä¸šã€æŸ¥çœ‹æ—¥å¿—ã€‚      | é€‚åˆè¿ç»´äººå‘˜å¿«é€Ÿæ“ä½œã€‚          |
| **IDE/API**      | å¼€å‘è€…åœ¨ä»£ç ä¸­ç›´æ¥è°ƒç”¨ Flink APIï¼ˆDataStream/Table APIï¼‰æ„å»ºä½œä¸šé€»è¾‘ã€‚ | æ”¯æŒ Java/Scala/Pythonã€‚        |
| **REST API**     | æä¾› HTTP æ¥å£æäº¤ä½œä¸šã€è·å–é›†ç¾¤çŠ¶æ€ï¼Œé›†æˆåˆ°å¤–éƒ¨ç³»ç»Ÿï¼ˆå¦‚è°ƒåº¦å¹³å°ï¼‰ã€‚ | é€‚åˆè‡ªåŠ¨åŒ–è¿ç»´åœºæ™¯ã€‚            |
| **Flink Client** | å°†ç”¨æˆ·ä»£ç ç¼–è¯‘ä¸º**é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼ˆJobGraphï¼‰**ï¼Œæäº¤ç»™ JobManagerã€‚ | ä¼˜åŒ– DAG ç»“æ„ï¼ˆå¦‚ç®—å­é“¾åˆå¹¶ï¼‰ã€‚ |

**2. é›†ç¾¤èµ„æºç®¡ç†å™¨**

| ç»„ä»¶           | åŠŸèƒ½èŒè´£                                      | é€‚ç”¨åœºæ™¯                               |
| -------------- | --------------------------------------------- | -------------------------------------- |
| **YARN**       | åŠ¨æ€åˆ†é…å®¹å™¨èµ„æºï¼Œç®¡ç† TaskManager ç”Ÿå‘½å‘¨æœŸã€‚ | Hadoop ç”Ÿæ€é›†æˆï¼Œèµ„æºåˆ©ç”¨ç‡é«˜ã€‚        |
| **Kubernetes** | åŸç”Ÿå®¹å™¨åŒ–éƒ¨ç½²ï¼Œæ”¯æŒå¼¹æ€§æ‰©ç¼©å®¹å’Œæ•…éšœæ¢å¤ã€‚    | äº‘åŸç”Ÿç¯å¢ƒï¼ˆå¦‚ AWS EKSã€é˜¿é‡Œäº‘ ACKï¼‰ã€‚ |
| **Standalone** | Flink å†…ç½®çš„ç®€å•èµ„æºç®¡ç†æ¨¡å¼ï¼Œæ— éœ€å¤–éƒ¨ä¾èµ–ã€‚  | æµ‹è¯•/å¼€å‘ç¯å¢ƒå¿«é€Ÿå¯åŠ¨ã€‚                |

åº•å±‚èµ„æºå¹³å°ï¼ˆå¦‚ YARNã€Kubernetesã€Mesosï¼‰ï¼Œå®ç°æä¾›ç‰©ç†èµ„æºï¼ˆCPU/å†…å­˜ï¼‰ï¼Œå¹¶æŒ‰éœ€å¯åŠ¨/é”€æ¯å®¹å™¨ï¼ˆContainer/Podï¼‰

**3. Flink è¿è¡Œæ—¶æ ¸å¿ƒç»„ä»¶**

| ç»„ä»¶                    | åŠŸèƒ½èŒè´£                                                     | å…³é”®æœºåˆ¶                                      |
| ----------------------- | ------------------------------------------------------------ | --------------------------------------------- |
| **JobManager**          | é›†ç¾¤ä¸»èŠ‚ç‚¹ï¼Œåè°ƒä½œä¸šæ‰§è¡Œçš„æ ¸å¿ƒç»„ä»¶ã€‚                         | é«˜å¯ç”¨éœ€ä¾èµ– ZooKeeperã€‚                      |
| å­ç»„ä»¶: ResourceManager | ç®¡ç† TaskManager èµ„æºæ± ï¼Œä¸å¤–éƒ¨èµ„æºæ¡†æ¶ï¼ˆYARN/K8sï¼‰äº¤äº’ã€‚    | æ”¯æŒ Slot åŠ¨æ€åˆ†é…ã€‚                          |
| å­ç»„ä»¶:  Dispatcher     | æ¥æ”¶å®¢æˆ·ç«¯ä½œä¸šæäº¤ï¼Œå¯åŠ¨ JobMasterï¼Œæä¾› Web UIã€‚            | å¤šç§Ÿæˆ·ç¯å¢ƒä¸‹è·¯ç”±ä¸åŒä½œä¸šåˆ°å¯¹åº” JobMasterã€‚    |
| å­ç»„ä»¶:  JobMaster      | ç®¡ç†å•ä¸ªä½œä¸šçš„ç”Ÿå‘½å‘¨æœŸï¼ˆç”Ÿæˆç‰©ç†æ‰§è¡Œå›¾ã€è°ƒåº¦ Taskã€æ•…éšœæ¢å¤ï¼‰ã€‚ | æ¯ä¸ªä½œä¸šç‹¬ç«‹ JobMasterã€‚                      |
| **TaskManager**         | å·¥ä½œèŠ‚ç‚¹ï¼Œæ‰§è¡Œå…·ä½“ Taskï¼Œç®¡ç†å†…å­˜çŠ¶æ€å’Œç½‘ç»œé€šä¿¡ã€‚            | é€šè¿‡ Slot åˆ’åˆ†èµ„æºã€‚                          |
| å­ç»„ä»¶:  Task Slot      | èµ„æºéš”ç¦»å•å…ƒï¼Œä¸€ä¸ª Slot å¯è¿è¡Œå¤šä¸ªç®—å­é“¾ï¼ˆå…±äº« CPU/å†…å­˜ï¼‰ã€‚  | æé«˜èµ„æºåˆ©ç”¨ç‡ï¼ˆå¦‚ 1 Slot è¿è¡Œ mapâ†’filterï¼‰ã€‚ |
| å­ç»„ä»¶:  Network Stack  | å¤„ç† TaskManager é—´çš„æ•°æ®ä¼ è¾“ï¼ˆShuffleï¼‰ã€åå‹æ§åˆ¶ã€é›¶æ‹·è´ä¼˜åŒ–ã€‚ | å…³é”®å½±å“ååå’Œå»¶è¿Ÿã€‚                          |

**4. å­˜å‚¨ç³»ç»Ÿ**

| ç»„ä»¶         | åŠŸèƒ½èŒè´£                                         | å…¸å‹ç”¨é€”                           |
| ------------ | ------------------------------------------------ | ---------------------------------- |
| **HDFS**     | å­˜å‚¨ Checkpoint/Savepoint çŠ¶æ€å¿«ç…§ï¼Œæ”¯æŒé«˜å®¹é”™ã€‚ | å¤§æ•°æ®ç”Ÿæ€é›†æˆï¼ˆå¦‚ Hadoop ç¯å¢ƒï¼‰ã€‚ |
| **S3**       | äº‘åŸç”Ÿå­˜å‚¨ï¼Œé€‚åˆå­˜æ”¾å¤§è§„æ¨¡çŠ¶æ€æ•°æ®ã€‚             | AWS/é˜¿é‡Œäº‘ç­‰äº‘å¹³å°ã€‚               |
| **æœ¬åœ°ç£ç›˜** | ä¸´æ—¶å­˜å‚¨çŠ¶æ€æ•°æ®ï¼Œæ€§èƒ½é«˜ä½†å¯é æ€§ä½ã€‚             | æµ‹è¯•ç¯å¢ƒæˆ–éå…³é”®ä½œä¸šã€‚             |



### 2.2Flinkæ¶æ„å·¥ä½œæµç¨‹

| ä½œä¸šé˜¶æ®µ                      | å‚ä¸ç»„ä»¶                                                     | æµç¨‹æ­¥éª¤                                                     |
| ----------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **1. ä½œä¸šæäº¤é˜¶æ®µ**           | å®¢æˆ·ç«¯ï¼ˆCLI/IDE/REST APIï¼‰ã€Flink Clientã€Dispatcher         | 1. ç”¨æˆ·é€šè¿‡å®¢æˆ·ç«¯æäº¤ä½œä¸šåˆ° Flink Clientã€‚ <br />2. Flink Client å°†ä½œä¸šï¼ˆJAR/SQLï¼‰ä¼ é€’ç»™ Dispatcherï¼ˆJobManager å…¥å£ï¼‰ã€‚ |
| **2. ä½œä¸šåˆå§‹åŒ–é˜¶æ®µ**         | Dispatcherã€JobMasterã€ResourceManager                       | 3. Dispatcher åˆ›å»º JobMasterï¼ˆæ¯ä¸ªä½œä¸šä¸€ä¸ªå®ä¾‹ï¼‰ã€‚<br />4. JobMaster è§£æä½œä¸š DAGï¼Œå¹¶å‘ ResourceManager ç”³è¯·èµ„æºï¼ˆSlotï¼‰ã€‚ |
| **3. èµ„æºåˆ†é…é˜¶æ®µ**           | ResourceManagerã€é›†ç¾¤èµ„æºç®¡ç†å™¨ï¼ˆYARN/K8s/Standaloneï¼‰ã€TaskManager | 5. ResourceManager ä¸åº•å±‚é›†ç¾¤åå•†ï¼Œå¯åŠ¨ TaskManager å®¹å™¨/Podã€‚ <br />6. TaskManager å¯åŠ¨åå‘ ResourceManager æ³¨å†Œ Slotï¼ˆèµ„æºä¿¡æ¯ï¼‰ã€‚ |
| **4. ä»»åŠ¡è°ƒåº¦ä¸æ‰§è¡Œé˜¶æ®µ**     | JobMasterã€TaskManager                                       | 7. JobMaster å°† Task åˆ†é…åˆ° TaskManager çš„ Slot ä¸Šã€‚ <br />8. TaskManager æ‰§è¡Œ Taskï¼Œå¹¶ä¸å…¶ä»– TaskManager äº¤æ¢æ•°æ®ï¼ˆå¦‚ Shuffleï¼‰ã€‚ |
| **5. æ•°æ®è¯»å†™é˜¶æ®µ**           | TaskManagerã€å­˜å‚¨ç³»ç»Ÿï¼ˆHDFS/S3/æœ¬åœ°ç£ç›˜ï¼‰                    | 9. TaskManager ä»å¤–éƒ¨å­˜å‚¨ï¼ˆå¦‚ HDFS/Kafkaï¼‰è¯»å–è¾“å…¥æ•°æ®ã€‚ <br />10. å¤„ç†å®Œæˆåå°†ç»“æœå†™å…¥å­˜å‚¨ç³»ç»Ÿï¼ˆå¦‚ S3/æ•°æ®åº“ï¼‰ã€‚ |
| **6. å®¹é”™ä¸åè°ƒ**ï¼ˆéšå«æµç¨‹ï¼‰ | JobMasterã€TaskManagerã€æ£€æŸ¥ç‚¹å­˜å‚¨                           | - JobMasterå®šæœŸè§¦å‘ Checkpointï¼ŒTaskManager ä¿å­˜çŠ¶æ€å¿«ç…§ã€‚ <br />- æ•…éšœæ—¶ï¼ŒJobMaster é‡æ–°è°ƒåº¦ Task å¹¶ä»æ£€æŸ¥ç‚¹æ¢å¤çŠ¶æ€ã€‚ |

```mermaid
graph TD
    subgraph "å®¢æˆ·ç«¯"
        A[CLIå‘½ä»¤è¡Œ] --> B[Flink Client]
        C[IDE/API] --> B
        D[REST API] --> B
    end
    
    subgraph "é›†ç¾¤èµ„æºç®¡ç†å™¨"
        E[YARN]
        F[Kubernetes]
        G[Standalone]
    end
    
    subgraph "Flinkè¿è¡Œæ—¶"
        H[JobManager] -->|èµ„æºè¯·æ±‚| E
        H -->|èµ„æºè¯·æ±‚| F
        H -->|èµ„æºè¯·æ±‚| G
        H --> I[ResourceManager]
        H --> J[Dispatcher]
        J -->|å¯åŠ¨| K[JobMaster]
        
        L[TaskManager1] -->|æ³¨å†Œ| I
        M[TaskManager2] -->|æ³¨å†Œ| I
        N[TaskManager3] -->|æ³¨å†Œ| I
        
        K -->|ä»»åŠ¡åˆ†é…| L
        K -->|ä»»åŠ¡åˆ†é…| M
        K -->|ä»»åŠ¡åˆ†é…| N
        
        L <-->|æ•°æ®äº¤æ¢| M
        M <-->|æ•°æ®äº¤æ¢| N
        N <-->|æ•°æ®äº¤æ¢| L
    end
    
    subgraph "å­˜å‚¨ç³»ç»Ÿ"
        O[HDFS]
        P[S3]
        Q[æœ¬åœ°ç£ç›˜]
    end
    
    B -->|æäº¤ä½œä¸š| J
    J -->|ä½œä¸šåˆ†å‘| K
    L -->|è¯»å†™æ•°æ®| O
    M -->|è¯»å†™æ•°æ®| P
    N -->|è¯»å†™æ•°æ®| Q      
```

### 2.3Flinkè¿è¡Œæ¨¡å¼

#### 1.æŒ‰éƒ¨ç½²ç¯å¢ƒåˆ’åˆ†

| ç¯å¢ƒ   | æ¨¡å¼ç±»å‹        | ç‰¹ç‚¹                                                         | åº”ç”¨åœºæ™¯                                                   | æ¶‰åŠå·¥å…·                      |
| ------ | --------------- | ------------------------------------------------------------ | ---------------------------------------------------------- | ----------------------------- |
| æœ¬åœ°   | æœ¬åœ°æ¨¡å¼        | å• JVM è¿›ç¨‹è¿è¡Œï¼Œæ— åˆ†å¸ƒå¼é›†ç¾¤ï¼Œè½»é‡çº§ï¼Œé€‚åˆå¼€å‘è°ƒè¯•          | å¼€å‘æµ‹è¯•ã€æ•™å­¦æ¼”ç¤º                                         | æ— ï¼ˆä»…éœ€ Flink è¿è¡Œæ—¶ç¯å¢ƒï¼‰   |
| é›†ç¾¤   | ç‹¬ç«‹é›†ç¾¤æ¨¡å¼    | ä¸“ç”¨ Flink é›†ç¾¤ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨èµ„æºç®¡ç†å™¨ï¼Œæ‰‹åŠ¨ç®¡ç†ï¼Œå›ºå®šèµ„æº  | å°è§„æ¨¡ç”Ÿäº§ç¯å¢ƒã€æµ‹è¯•ç¯å¢ƒã€èµ„æºéš”ç¦»éœ€æ±‚é«˜çš„åœºæ™¯             | Flink è‡ªèº«ç®¡ç†å·¥å…·            |
| é›†ç¾¤   | YARN ä¼šè¯æ¨¡å¼   | ä¸ Hadoop YARN é›†æˆï¼Œå…±äº«é›†ç¾¤èµ„æºï¼Œé¢„å…ˆå¯åŠ¨é›†ç¾¤ï¼Œå¤šä½œä¸šå…±äº«èµ„æº | å·²æœ‰ Hadoop é›†ç¾¤ã€éœ€ç»Ÿä¸€èµ„æºç®¡ç†ã€æ‰¹æµæ··åˆéƒ¨ç½²             | Hadoop YARNã€Flink            |
| é›†ç¾¤   | YARN åº”ç”¨æ¨¡å¼   | ä¸ Hadoop YARN é›†æˆï¼Œæ¯ä¸ªä½œä¸šä¸€ä¸ªç‹¬ç«‹é›†ç¾¤ï¼Œèµ„æºéš”ç¦»æ€§å¥½      | èµ„æºéš”ç¦»è¦æ±‚é«˜çš„ä½œä¸š                                       | Hadoop YARNã€Flink            |
| é›†ç¾¤   | Kubernetes æ¨¡å¼ | äº‘åŸç”Ÿéƒ¨ç½²ï¼Œå®¹å™¨åŒ–ç®¡ç†ï¼Œæ”¯æŒå¼¹æ€§ä¼¸ç¼©å’Œå£°æ˜å¼é…ç½®             | äº‘ç¯å¢ƒï¼ˆå¦‚ AWS EKSã€Google GKEï¼‰ã€DevOps æµç¨‹é›†æˆï¼ˆCI/CDï¼‰ | Kubernetesã€Dockerã€Flink     |
| äº‘æœåŠ¡ | äº‘æ‰˜ç®¡æœåŠ¡      | å®Œå…¨æ‰˜ç®¡ï¼Œæ— éœ€ç®¡ç†åŸºç¡€è®¾æ–½ï¼Œä¸äº‘ç”Ÿæ€æ·±åº¦é›†æˆ                 | äº‘ç¯å¢ƒä¸‹æ— éœ€ç®¡ç†åŸºç¡€è®¾æ–½ã€éœ€å¿«é€Ÿæ­å»ºå®æ—¶æ•°æ®å¤„ç†çš„åœºæ™¯     | å„äº‘æœåŠ¡å•†å¹³å°ï¼ˆAWSã€GCP ç­‰ï¼‰ |
| åµŒå…¥   | åµŒå…¥æ¨¡å¼        | Flink ä½œä¸ºåº“é›†æˆåˆ°åº”ç”¨ç¨‹åºä¸­ï¼Œæ— ç‹¬ç«‹é›†ç¾¤ï¼Œä¸åº”ç”¨å…±äº« JVM èµ„æº | å¾®æœåŠ¡ä¸­é›†æˆå®æ—¶è®¡ç®—åŠŸèƒ½ã€èµ„æºå—é™ç¯å¢ƒä¸‹çš„è½»é‡çº§æµå¤„ç†     | Flink è¿è¡Œæ—¶åº“                |

| **æ¨¡å¼**      | **èµ„æºç®¡ç†**    | **å¯åŠ¨é€Ÿåº¦** | **èµ„æºåˆ©ç”¨ç‡** | **éš”ç¦»æ€§** | **é€‚ç”¨åœºæ™¯**           |
| ------------- | --------------- | ------------ | -------------- | ---------- | ---------------------- |
| æœ¬åœ°æ¨¡å¼      | å• JVM          | æå¿«         | ä½             | æ—          | å¼€å‘è°ƒè¯•               |
| Standalone    | ç‹¬ç«‹ Flink é›†ç¾¤ | è¾ƒæ…¢         | ä¸­             | ä¸­         | ä¸“ç”¨é›†ç¾¤ã€å°è§„æ¨¡ç”Ÿäº§   |
| YARN ä¼šè¯æ¨¡å¼ | Hadoop YARN     | ä¸­ç­‰         | é«˜             | ä½         | å…±äº«èµ„æºã€å¤šä½œä¸šè°ƒåº¦   |
| YARN åº”ç”¨æ¨¡å¼ | Hadoop YARN     | è¾ƒæ…¢         | é«˜             | é«˜         | èµ„æºéš”ç¦»è¦æ±‚é«˜çš„ä½œä¸š   |
| Kubernetes    | å®¹å™¨ç¼–æ’å¹³å°    | ä¸­ç­‰         | é«˜             | é«˜         | äº‘åŸç”Ÿã€å¼¹æ€§ä¼¸ç¼©       |
| äº‘æ‰˜ç®¡æœåŠ¡    | äº‘æœåŠ¡å•†ç®¡ç†    | å¿«           | é«˜             | é«˜         | æ— éœ€ç®¡ç†åŸºç¡€è®¾æ–½çš„åœºæ™¯ |
| åµŒå…¥æ¨¡å¼      | åº”ç”¨ç¨‹åºè‡ªèº«    | å¿«           | ä½             | ä½         | å¾®æœåŠ¡é›†æˆã€åµŒå…¥å¼ç³»ç»Ÿ |

#### 2.æŒ‰é›†ç¾¤ç”Ÿå‘½å‘¨æœŸåˆ’åˆ†

**Session æ¨¡å¼(ä¼šè¯æ¨¡å¼ï¼‰**

Session æ¨¡å¼å‡è®¾å·²æœ‰ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„é›†ç¾¤ï¼Œå¹¶ä½¿ç”¨è¯¥é›†ç¾¤çš„èµ„æºæ¥æ‰§è¡Œä»»ä½•æäº¤çš„åº”ç”¨ç¨‹åºã€‚

ä¼˜ç‚¹: å…±äº«é›†ç¾¤èµ„æºï¼Œé¿å…äº†ä¸ºæ¯ä¸ªæäº¤çš„ä½œä¸šå¯åŠ¨ä¸€ä¸ªå®Œæ•´é›†ç¾¤æ‰€å¸¦æ¥çš„èµ„æºå¼€é”€ã€‚

ç¼ºç‚¹ï¼šèµ„æºéš”ç¦»æ€§å·®ï¼Œå¤šä¸ªä½œä¸šç«äº‰ Slotï¼Œèµ„æºåˆ†é…åŠ¨æ€è°ƒæ•´

**Per-Job æ¨¡å¼ï¼ˆå•ä½œä¸šæ¨¡å¼ï¼‰å·²åºŸå¼ƒ** 

æ¯ä¸ªä½œä¸šç‹¬å é›†ç¾¤ , å¼ºèµ„æºéš”ç¦» , ä½œä¸šå®Œæˆå³é‡Šæ”¾èµ„æº

**Application æ¨¡å¼ï¼ˆåº”ç”¨æ¨¡å¼ï¼‰** 

åº”ç”¨çº§èµ„æºå…±äº«ï¼Œå¤šä½œä¸šåä½œï¼Œä¾èµ–åŒ…åœ¨é›†ç¾¤å†…å…±äº«

**ä¼˜ç‚¹**: é¿å…ä¾èµ–é‡å¤ä¸Šä¼  

**ç¼ºç‚¹**:  ä»…æ”¯æŒ Flink 1.11+



## 3.Flinkå¼€å‘ç¯å¢ƒæ­å»º

### 3.1å•æœºæ¨¡å¼(å¼€å‘æµ‹è¯•)

#### 1.æ­å»ºflinkå•æœºç¯å¢ƒ

```bash
# å¯åŠ¨JobManager + Web UI
docker run -d --name flink-jobmanager \
  -p 8081:8081 \
  -e FLINK_PROPERTIES="jobmanager.rpc.address=jobmanager" \
  apache/flink:1.17.1-scala_2.12-java11 jobmanager

# å¯åŠ¨TaskManagerï¼ˆè¿æ¥åˆ°JobManagerï¼‰
docker run -d --name flink-taskmanager \
  --link flink-jobmanager:jobmanager \
  -e FLINK_PROPERTIES="jobmanager.rpc.address=jobmanager" \
  apache/flink:1.17.1-scala_2.12-java11 taskmanager
```

å‘½ä»¤è¯´æ˜ï¼š

- jobmanager.rpc.addressï¼š JobManager è¿œç¨‹è¿‡ç¨‹è°ƒç”¨æœåŠ¡åœ°å€, å…¶ä»–ç»„ä»¶ï¼ˆå¦‚ TaskManagerã€å®¢æˆ·ç«¯ï¼‰éƒ½éœ€è¦é€šè¿‡ RPC åœ°å€ä¸ JobManager é€šä¿¡

- å‘½ä»¤æœ«å°¾çš„ `jobmanager` æˆ– `taskmanager`:  æŒ‡å®š **Flink å®¹å™¨çš„è¿è¡Œè§’è‰²**, ä¸º jobmanager æˆ– taskmanager

éªŒè¯æ˜¯å¦å®‰è£…æˆåŠŸï¼š

- è®¿é—® `http://localhost:8081`
- æŸ¥çœ‹æ—¥å¿—ï¼š`docker logs flink-jobmanager`

#### 2.å…¥é—¨æ¡ˆä¾‹: WordCountç¤ºä¾‹

`apache/flink:1.17.1-scala_2.12-java11`å¹¶æ²¡æœ‰pythonç¯å¢ƒï¼Œåœ¨



### 3.2æ„å»ºpython pyFlink ç¯å¢ƒé•œåƒ

docker build -t flink-1.17.1-py3.9:latest

```dockerfile
FROM --platform=linux/amd64 apache/flink:1.17.1-scala_2.12-java11

# å®‰è£…åŸºç¡€ä¾èµ–
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        python3 \
        python3-pip \
        python3-dev \
        libgomp1 && \
    rm -rf /var/lib/apt/lists/* && \
    ln -sf /usr/bin/python3 /usr/bin/python && \
    ln -sf /usr/bin/pip3 /usr/bin/pip

# å®‰è£…PyFlinkåŠä¾èµ–ï¼ˆä¿®æ­£æ ¼å¼ï¼‰
RUN pip install --upgrade pip && \
    pip install \
        apache-flink==1.17.1 \
        pyarrow==7.0.0 \
        pandas==1.3.5 \
        -i https://pypi.tuna.tsinghua.edu.cn/simple

# ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/opt/flink/opt/python
ENV PYFLINK_CLIENT_EXECUTABLE=python

```



```dockerfile
# ç§»é™¤ç¡¬ç¼–ç --platformï¼Œä½¿ç”¨åŠ¨æ€æ„å»ºå‚æ•°
FROM apache/flink:1.17.1-scala_2.12-java11

# å®‰è£…åŸºç¡€ä¾èµ–ï¼ˆä½¿ç”¨é˜¿é‡Œäº‘apté•œåƒåŠ é€Ÿï¼‰
RUN sed -i 's/deb.debian.org/mirrors.aliyun.com/g' /etc/apt/sources.list && \
    apt-get update && \
    apt-get install -y --no-install-recommends \
        python3 \
        python3-pip \
        python3-dev \
        libgomp1 && \
    rm -rf /var/lib/apt/lists/* && \
    ln -sf /usr/bin/python3 /usr/bin/python

# å®‰è£…Pythonä¾èµ–ï¼ˆæ¸…åé•œåƒ+å¹¶è¡Œä¼˜åŒ–ï¼‰
RUN pip install --upgrade pip && \
    pip install \
        --index-url https://pypi.tuna.tsinghua.edu.cn/simple \
        --trusted-host pypi.tuna.tsinghua.edu.cn \
        --no-cache-dir \
        apache-flink==1.17.1 \
        pyarrow==7.0.0 \
        pandas==1.3.5

# ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/opt/flink/opt/python \
    PYFLINK_CLIENT_EXECUTABLE=python

```



### 3.3 flink-scala_2.12-java11-py3.8è¯´æ˜	

#### åˆ›å»º DataStream API æ•°æ®æµå¤„ç†ç¯å¢ƒ

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment
data_stream_env = StreamExecutionEnvironment.get_execution_environment()
data_stream_table_env = StreamTableEnvironment.create(data_stream_env)
```

#### åˆ›å»º Table API æµå¤„ç†è¡¨ç¯å¢ƒ

```python
from pyflink.table import EnvironmentSettings, TableEnvironment
stream_env_settings = EnvironmentSettings.in_streaming_mode()
stream_table_env = TableEnvironment.create(stream_env_settings)
```

#### åˆ›å»ºTable API æ‰¹å¤„ç†è¡¨ç¯å¢ƒ

```python
from pyflink.table import EnvironmentSettings, TableEnvironment
batch_env_settings = EnvironmentSettings.in_batch_mode()
batch_table_env = TableEnvironment.create(batch_env_settings)
```

#### **JSON æ–‡ä»¶è¯»å–**

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors.file_system import FileSource
from pyflink.datastream.formats.json import JsonRowDeserializationSchema
from pyflink.common import Types, WatermarkStrategy

env = StreamExecutionEnvironment.get_execution_environment()
env.add_jars("file:///path/to/flink-connector-files-1.17.0.jar")  # å¿…é¡»æ·»åŠ 

# å®šä¹‰æ•°æ®ç±»å‹
row_type = Types.ROW_NAMED(
    ["name", "age", "city"],
    [Types.STRING(), Types.INT(), Types.STRING()]
)

# åˆ›å»º JSON ååºåˆ—åŒ–å™¨
json_deserializer = JsonRowDeserializationSchema.Builder() \
    .type_info(row_type) \
    .build()

# æ„å»º FileSource
source = FileSource.for_record_stream_format(
    json_deserializer,  # ç›´æ¥ä½¿ç”¨ååºåˆ—åŒ–å™¨
    "file:///path/to/input.json"
).build()

ds = env.from_source(source, WatermarkStrategy.no_watermarks(), "json-source")
ds.print()
env.execute()

```

#### **CSV æ–‡ä»¶è¯»å–**

```python
from pyflink.datastream.formats.csv import CsvReaderFormat

csv_format = CsvReaderFormat.for_row_type(
    row_type=Types.ROW_NAMED(
        ["name", "age"],
        [Types.STRING(), Types.INT()]
    ),
    field_delimiter=","
)

source = FileSource.for_stream_format(
    csv_format,
    "file:///path/to/input.csv"
).build()
```

#### **FileSink ç¤ºä¾‹å†™å…¥æ–‡ä»¶**

```
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors.file_system import FileSink
from pyflink.common.serialization import Encoder
from pyflink.common import Types

env = StreamExecutionEnvironment.get_execution_environment()
env.add_jars("file:///path/to/flink-connector-files-1.17.0.jar")  # å¿…é¡»ä¾èµ–

# ç¤ºä¾‹æ•°æ®æµ
ds = env.from_collection([("Alice", 25), ("Bob", 30)], Types.ROW([Types.STRING(), Types.INT()]))

# å®šä¹‰ FileSink
sink = FileSink.for_row_format(
    "file:///path/to/output",
    Encoder.simple_string_encoder()
).build()

ds.sink_to(sink)
env.execute("FileSink Example")
```

#### **KafkaSourceæˆ–KafkaSink**

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors.kafka import KafkaSource, KafkaSink
from pyflink.common.serialization import JsonRowDeserializationSchema, JsonRowSerializationSchema
from pyflink.common import Types, WatermarkStrategy

env = StreamExecutionEnvironment.get_execution_environment()
env.add_jars(
    "file:///path/to/flink-connector-kafka-1.17.0.jar",
    "file:///path/to/flink-json-1.17.0.jar"
)

# KafkaSourceï¼ˆæ¶ˆè´¹æ•°æ®ï¼‰
row_type = Types.ROW_NAMED(["name", "age"], [Types.STRING(), Types.INT()])
kafka_source = KafkaSource.builder() \
    .set_bootstrap_servers("localhost:9092") \
    .set_topics("input-topic") \
    .set_value_only_deserializer(
        JsonRowDeserializationSchema.Builder().type_info(row_type).build()
    ) \
    .build()

# KafkaSinkï¼ˆç”Ÿäº§æ•°æ®ï¼‰
serialization_schema = JsonRowSerializationSchema.Builder().with_type_info(row_type).build()
kafka_sink = KafkaSink.builder() \
    .set_bootstrap_servers("localhost:9092") \
    .set_record_serializer(
        KafkaRecordSerializationSchema.builder()
            .set_topic("output-topic")
            .set_value_serialization_schema(serialization_schema)
            .build()
    ) \
    .build()

# æ„å»ºæµæ°´çº¿
ds = env.from_source(kafka_source, WatermarkStrategy.no_watermarks(), "Kafka Source")
ds.sink_to(kafka_sink)
env.execute("Kafka Example")

```

#### **Table API è¿æ¥å™¨ï¼ˆDDL æ–¹å¼ï¼‰**

```python
from pyflink.table import EnvironmentSettings, TableEnvironment

t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())

# Kafka Source Table
t_env.execute_sql("""
    CREATE TABLE kafka_source (
        name STRING,
        age INT
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'input-topic',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

# FileSystem Sink Table
t_env.execute_sql("""
    CREATE TABLE fs_sink (
        name STRING,
        age INT
    ) WITH (
        'connector' = 'filesystem',
        'path' = 'file:///path/to/output',
        'format' = 'csv'
    )
""")

# æ‰§è¡Œ ETL
t_env.execute_sql("INSERT INTO fs_sink SELECT * FROM kafka_source")

```



### 3.4Flink1.17.1éƒ¨ç½²JDBCè¿æ¥

éœ€è¦çš„jaråŒ…å¦‚ä¸‹ï¼š

[mysql-connector-java-8.0.27.jar](https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar)
[flink-connector-jdbc-3.1.1-1.17.jar](https://repo1.maven.org/maven2/org/apache/flink/flink-connector-jdbc/3.1.1-1.17/flink-connector-jdbc-3.1.1-1.17.jar)



## 4.Flinkç¼–ç¨‹æ¨¡å‹

### 4.1Flink APIåˆé›†åŠé€‚ç”¨åœºæ™¯

| API åç§°               | åŠŸèƒ½æˆ–ä½œç”¨                                                   | é€‚ç”¨åœºæ™¯                                                     |
| ---------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **DataStream API**     | æä¾›ä½é˜¶æµå¤„ç†æ“ä½œï¼ˆmap/filter/windowç­‰ï¼‰ï¼Œæ”¯æŒç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰å’ŒçŠ¶æ€ç®¡ç† | å®æ—¶æµå¤„ç†ï¼ˆå¦‚ç‚¹å‡»æµåˆ†æã€å®æ—¶é£æ§ï¼‰ã€éœ€è¦ç²¾ç»†æ§åˆ¶æ—¶é—´å’ŒçŠ¶æ€çš„åœºæ™¯ |
| **DataSet API**        | æ‰¹å¤„ç†æ“ä½œï¼ˆå·²åºŸå¼ƒï¼Œæ¨èä½¿ç”¨ Table API æ›¿ä»£ï¼‰                | ç¦»çº¿æ‰¹å¤„ç†ï¼ˆå†å²æ•°æ®åˆ†æï¼‰ï¼ŒFlink 1.12+ åä¸æ¨èæ–°é¡¹ç›®ä½¿ç”¨   |
| **Table API & SQL**    | å£°æ˜å¼ APIï¼Œç»Ÿä¸€æµæ‰¹å¤„ç†ï¼Œæ”¯æŒ ANSI SQL å’Œå…³ç³»å‹æ“ä½œ         | å¿«é€Ÿå¼€å‘åˆ†æåº”ç”¨ï¼ˆå¦‚ETLï¼‰ã€éœ€è¦æµæ‰¹ä¸€ä½“åŒ–çš„åœºæ™¯ã€SQL å…¼å®¹éœ€æ±‚ |
| **Stateful Functions** | è·¨æœåŠ¡çš„çŠ¶æ€ç®¡ç†ï¼Œæ”¯æŒäº‹ä»¶é©±åŠ¨æ¶æ„å’Œæ¶ˆæ¯è·¯ç”±                 | å¾®æœåŠ¡åº”ç”¨ã€åˆ†å¸ƒå¼äº‹åŠ¡å¤„ç†ã€éœ€è¦æŒä¹…åŒ–è·¨å‡½æ•°çŠ¶æ€çš„åœºæ™¯       |
| **CEP API**            | å¤æ‚äº‹ä»¶æ¨¡å¼æ£€æµ‹ï¼ˆå¦‚è¿ç»­äº‹ä»¶åºåˆ—åŒ¹é…ï¼‰                       | é£é™©ç›‘æ§ï¼ˆå¦‚å¼‚å¸¸ç™»å½•æ£€æµ‹ï¼‰ã€ç‰©è”ç½‘è®¾å¤‡å¼‚å¸¸æ¨¡å¼è¯†åˆ«           |
| **PyFlink API**        | Python æ¥å£ï¼Œæ”¯æŒ DataStream å’Œ Table API                    | Python ç”Ÿæ€é›†æˆã€æœºå™¨å­¦ä¹ ç®¡é“ã€å¿«é€ŸåŸå‹å¼€å‘                  |
| **Process Function**   | æœ€åº•å±‚ APIï¼Œå¯è®¿é—®æ—¶é—´æˆ³ã€æ°´ä½çº¿å’ŒçŠ¶æ€åç«¯                   | éœ€è¦è‡ªå®šä¹‰å¤æ‚å¤„ç†é€»è¾‘ï¼ˆå¦‚è‡ªå®šä¹‰çª—å£ï¼‰ã€ä¸å¤–éƒ¨ç³»ç»Ÿæ·±åº¦é›†æˆçš„åœºæ™¯ |
| **Connector API**      | ä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’çš„æºï¼ˆSourceï¼‰å’Œæ±‡ï¼ˆSinkï¼‰å®ç°                 | å¯¹æ¥ Kafka/HDFS/JDBC ç­‰å¤–éƒ¨ç³»ç»Ÿã€è‡ªå®šä¹‰æ•°æ®æº/ç›®çš„åœ°         |
| **Metrics API**        | æš´éœ²è¿è¡Œæ—¶æŒ‡æ ‡ï¼ˆååé‡ã€å»¶è¿Ÿç­‰ï¼‰                             | æ€§èƒ½ç›‘æ§ã€è‡ªåŠ¨åŒ–æ‰©ç¼©å®¹ã€ä½œä¸šè°ƒä¼˜                             |

### 4.2DataStream API æ ¸å¿ƒç»„ä»¶

#### 1.DataStream APIæ ¸å¿ƒä½œç”¨

- å¤„ç†æ— ç•Œæ•°æ®æµ: æ”¯æŒå®æ—¶å¤„ç†æºæºä¸æ–­çš„æ•°æ®æµï¼ˆå¦‚æ—¥å¿—ã€ä¼ æ„Ÿå™¨æ•°æ®ã€ç”¨æˆ·è¡Œä¸ºç­‰ï¼‰ï¼Œè€Œéæ‰¹å¤„ç†é™æ€æ•°æ®é›†ã€‚
- ç²¾ç¡®çš„æ—¶é—´è¯­ä¹‰: æä¾›äº‹ä»¶æ—¶é—´ï¼ˆEvent Timeï¼‰ã€å¤„ç†æ—¶é—´ï¼ˆProcessing Timeï¼‰å’Œæ‘„å…¥æ—¶é—´ï¼ˆIngestion Timeï¼‰ä¸‰ç§æ—¶é—´è¯­ä¹‰ï¼Œè§£å†³æ•°æ®ä¹±åºé—®é¢˜ã€‚
- çŠ¶æ€ç®¡ç†ä¸å®¹é”™: æ”¯æŒè·¨ä»»åŠ¡ä¿å­˜å’Œæ¢å¤çŠ¶æ€ï¼ˆå¦‚è®¡æ•°å™¨ã€çª—å£èšåˆç»“æœï¼‰ï¼Œç»“åˆ Checkpoint æœºåˆ¶å®ç° Exactly-Once è¯­ä¹‰ã€‚
- ä½å»¶è¿Ÿä¸é«˜åå: é€šè¿‡ä¼˜åŒ–çš„æ‰§è¡Œå¼•æ“ï¼Œåœ¨ä¿è¯ä½å»¶è¿Ÿçš„åŒæ—¶å¤„ç†é«˜å¹¶å‘æ•°æ®ï¼ˆå¦‚æ¯ç§’ç™¾ä¸‡çº§äº‹ä»¶ï¼‰ã€‚

#### 2.è®¾ç½®æ‰§è¡Œç¯å¢ƒ

##### æ‰§è¡Œç¯å¢ƒä½œç”¨

- ç¨‹åºå…¥å£: **è®¾ç½®æ‰§è¡Œç¯å¢ƒï¼ˆExecution Environmentï¼‰æ˜¯æ‰€æœ‰ Flink ç¨‹åºçš„å…¥å£ç‚¹**
- é…ç½®æ‰§è¡Œå‚æ•°ï¼ˆå¹¶è¡Œåº¦ã€å®¹é”™ç­–ç•¥ï¼‰ã€‚

##### ä¸‰ç§æ‰§è¡Œç¯å¢ƒ

| **æ‰§è¡Œç¯å¢ƒ**                                 | **é€‚ç”¨åœºæ™¯**                                                 | **ç‰¹ç‚¹**                                                     |
| -------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `StreamExecutionEnvironment`<br />æµå¤„ç†ç¯å¢ƒ | - æ ‡å‡†æµå¤„ç†ç¯å¢ƒï¼ˆé»˜è®¤ï¼‰<br />- é€šç”¨åœºæ™¯ï¼ˆå¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ï¼‰ | - è‡ªåŠ¨æ£€æµ‹æ‰§è¡Œç¯å¢ƒï¼ˆæœ¬åœ°æˆ–é›†ç¾¤ï¼‰ <br />- æ”¯æŒæ‰¹æµä¸€ä½“ - ç”Ÿäº§ç¯å¢ƒå¸¸ç”¨ |
| `LocalEnvironment`<br />æœ¬åœ°è°ƒè¯•ç¯å¢ƒ         | æœ¬åœ°æµ‹è¯•/è°ƒè¯•                                                | - åœ¨æœ¬åœ° JVM ä¸­è¿è¡Œ - ä¸ä¾èµ–é›†ç¾¤ <br />- å¯è®¾ç½®å¹¶è¡Œåº¦æ¨¡æ‹Ÿåˆ†å¸ƒå¼è¡Œä¸º |
| `RemoteEnvironment`<br />è¿æ¥è¿œç¨‹é›†ç¾¤        | è¿æ¥è¿œç¨‹é›†ç¾¤æ‰§è¡Œ                                             | - æäº¤ä½œä¸šåˆ°è¿œç¨‹é›†ç¾¤ï¼ˆå¦‚ YARN/K8sï¼‰ <br />- éœ€æŒ‡å®šé›†ç¾¤åœ°å€å’Œ Jar åŒ…è·¯å¾„ |

##### å‚æ•°åŠç¤ºä¾‹

| **å‚æ•°/æ–¹æ³•**                                    | **ä½œç”¨**                                         | **é»˜è®¤å€¼**          | **ç¤ºä¾‹/æ¨èå€¼**                                           |
| ------------------------------------------------ | ------------------------------------------------ | ------------------- | --------------------------------------------------------- |
| `env.set_parallelism(n)`                         | è®¾ç½®ä½œä¸šå…¨å±€å¹¶è¡Œåº¦                               | 1                   | `set_parallelism(4)`                                      |
| `operator.set_parallelism(n)`                    | è®¾ç½®å•ä¸ªç®—å­å¹¶è¡Œåº¦ï¼ˆè¦†ç›–å…¨å±€ï¼‰                   | ç»§æ‰¿å…¨å±€å€¼          | `map(...).set_parallelism(2)`                             |
| `env.default_parallelism`                        | è·å–æˆ–è®¾ç½®é»˜è®¤å¹¶è¡Œåº¦ï¼ˆæœªæ˜¾å¼æŒ‡å®šæ—¶ç”Ÿæ•ˆï¼‰         | 1                   | `default_parallelism = 4`                                 |
| `env.enable_checkpointing(interval, mode)`       | å¼€å¯ Checkpointï¼ˆé—´éš”å’Œæ¨¡å¼ï¼‰                    | æœªå¯ç”¨              | `enable_checkpointing(10000, EXACTLY_ONCE)`               |
| `set_checkpointing_mode(mode)`                   | è®¾ç½®ä¸€è‡´æ€§æ¨¡å¼ï¼ˆ`EXACTLY_ONCE`/`AT_LEAST_ONCE`ï¼‰ | `EXACTLY_ONCE`      | `set_checkpointing_mode(AT_LEAST_ONCE)`                   |
| `set_checkpoint_timeout(timeout_ms)`             | Checkpoint è¶…æ—¶æ—¶é—´ï¼ˆè¶…æ—¶åˆ™ä¸¢å¼ƒï¼‰                | 600000 msï¼ˆ10åˆ†é’Ÿï¼‰ | `set_checkpoint_timeout(300000)`                          |
| `set_min_pause_between_checkpoints(interval_ms)` | ä¸¤æ¬¡ Checkpoint æœ€å°é—´éš”ï¼ˆé˜²é‡å ï¼‰               | 0ï¼ˆæ— é—´éš”ï¼‰         | `set_min_pause_between_checkpoints(5000)`                 |
| `set_max_concurrent_checkpoints(n)`              | æœ€å¤§å¹¶å‘ Checkpoint æ•°                           | 1                   | `set_max_concurrent_checkpoints(2)`                       |
| `enable_externalized_checkpoints(cleanup_mode)`  | å¤–éƒ¨åŒ– Checkpointï¼ˆä»»åŠ¡åœæ­¢åä¿ç•™ï¼‰              | æœªå¯ç”¨              | `enable_externalized_checkpoints(RETAIN_ON_CANCELLATION)` |
| `set_fail_on_checkpointing_errors(bool)`         | Checkpoint å¤±è´¥æ—¶æ˜¯å¦ä½¿ä»»åŠ¡å¤±è´¥                  | `True`              | `set_fail_on_checkpointing_errors(False)`                 |
| `state.backend`                                  | çŠ¶æ€åç«¯å®ç°ï¼ˆé€šè¿‡ `Configuration` è®¾ç½®ï¼‰        | æ—                   | `filesystem`/`rocksdb`                                    |
| `state.checkpoints.dir`                          | Checkpoint å­˜å‚¨è·¯å¾„                              | æ—                   | `file:///tmp/checkpoints`                                 |
| `state.backend.incremental`                      | æ˜¯å¦å¯ç”¨å¢é‡ Checkpointï¼ˆä»… RocksDBï¼‰            | `false`             | `config.set_boolean("...incremental", True)`              |
| `env.get_runtime_mode()`                         | è·å–æ‰§è¡Œæ¨¡å¼ï¼ˆ`STREAMING`/`BATCH`ï¼‰              | `STREAMING`         | `set_runtime_mode(BATCH)`                                 |
| `env.set_restart_strategy(...)`                  | è®¾ç½®æ•…éšœé‡å¯ç­–ç•¥                                 | æ— ç­–ç•¥              | `fixed_delay_restart(3, 10000)`                           |

```python
from pyflink.datastream import StreamExecutionEnvironment

env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(4)  # è®¾ç½®å¹¶è¡Œåº¦
```

##### Checkpointä½œç”¨

Flink Checkpoint æ˜¯ Flink å®ç°**å®¹é”™æœºåˆ¶**çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå®ƒå…è®¸ Flink åœ¨å‘ç”Ÿæ•…éšœï¼ˆå¦‚èŠ‚ç‚¹å´©æºƒã€ç½‘ç»œä¸­æ–­ï¼‰æ—¶ï¼Œ**è‡ªåŠ¨æ¢å¤ä½œä¸šçŠ¶æ€å¹¶ç»§ç»­å¤„ç†æ•°æ®**ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§å’Œå¤„ç†çš„å¯é æ€§

- ##### å·¥ç”¨åŸç†

| **é˜¶æ®µ**            | **å…³é”®æ­¥éª¤**                          | **è¯¦ç»†è¯´æ˜**                                                 | **å¯è§†åŒ–ç±»æ¯”**     |
| ------------------- | ------------------------------------- | ------------------------------------------------------------ | ------------------ |
| **1. è§¦å‘**         | JobManager å‘èµ·æŒ‡ä»¤                   | å®šæ—¶ï¼ˆæŒ‰ `interval`ï¼‰æˆ–æ‰‹åŠ¨è§¦å‘ Checkpoint ç”Ÿæˆè¯·æ±‚ã€‚        | â° å®šæ—¶é—¹é’Ÿ         |
| **2. Barrier æ³¨å…¥** | æ•°æ®æºæ’å…¥ Barrier                    | æ•°æ®æºï¼ˆå¦‚ Kafkaï¼‰åœ¨æµä¸­æ’å…¥ç‰¹æ®Šæ ‡è®°ï¼ˆBarrierï¼‰ï¼Œå°†æ•°æ®åˆ’åˆ†ä¸ºä¸åŒ Checkpoint æ‰¹æ¬¡ã€‚ | ğŸš§ æ°´æµä¸­çš„åˆ†éš”æµ®æ ‡ |
| **3. çŠ¶æ€å¿«ç…§**     | ç®—å­å¼‚æ­¥ä¿å­˜çŠ¶æ€                      | ç®—å­æ”¶åˆ° Barrier åï¼Œç«‹å³å°†å½“å‰çŠ¶æ€ï¼ˆå¦‚çª—å£è®¡æ•°ã€KV çŠ¶æ€ï¼‰å¼‚æ­¥å†™å…¥å­˜å‚¨åç«¯ã€‚ | ğŸ“¸ å¿«é€Ÿæ‹ç…§å­˜æ¡£     |
| **4. Barrier å¯¹é½** | å¤šè¾“å…¥ç®—å­ç­‰å¾…å¯¹é½ï¼ˆä»… Exactly-Onceï¼‰ | å¦‚ `join` éœ€ç­‰å¾…æ‰€æœ‰è¾“å…¥æµçš„ Barrier åˆ°è¾¾ï¼Œç¡®ä¿çŠ¶æ€ä¸€è‡´æ€§ã€‚  | ğŸš¦ å¤šè½¦é“åŒæ—¶äº®ç»¿ç¯ |
| **5. ç¡®è®¤å®Œæˆ**     | å‘ JobManager å‘é€ ACK                | æ‰€æœ‰ç®—å­å®Œæˆå¿«ç…§åï¼ŒJobManager æ ‡è®°è¯¥ Checkpoint ä¸ºæˆåŠŸï¼Œå¹¶è®°å½•å…ƒæ•°æ®ã€‚ | âœ… å…¨å‘˜ç­¾åˆ°å®Œæ¯•     |
| **6. æ•…éšœæ¢å¤**     | ä»æœ€è¿‘ Checkpoint æ¢å¤                | ä»»åŠ¡å¤±è´¥æ—¶ï¼Œé‡æ–°åŠ è½½çŠ¶æ€å¹¶ä» Barrier å¯¹åº”ä½ç½®é‡æ–°æ¶ˆè´¹æ•°æ®ï¼ˆå¦‚ Kafka Offsetï¼‰ã€‚ | âª æ¸¸æˆè¯»æ¡£ç»§ç»­     |

- **Checkpoint ä¸ Savepoint çš„åŒºåˆ«**

| **ç‰¹æ€§**     | **Checkpoint**                    | **Savepoint**                |
| :----------- | --------------------------------- | ---------------------------- |
| **ç›®çš„**     | è‡ªåŠ¨å®¹é”™ï¼ˆæ•…éšœæ¢å¤ï¼‰              | æ‰‹åŠ¨å¤‡ä»½ï¼ˆå‡çº§ã€è¿ç§»ã€è°ƒè¯•ï¼‰ |
| **è§¦å‘æ–¹å¼** | å®šæ—¶è‡ªåŠ¨è§¦å‘                      | ç”¨æˆ·æ‰‹åŠ¨è§¦å‘                 |
| **å­˜å‚¨æ ¼å¼** | å¯èƒ½ä¸ºå¢é‡/å†…éƒ¨æ ¼å¼ï¼ˆå¦‚ RocksDBï¼‰ | æ ‡å‡†åŒ–æ ¼å¼ï¼ˆå¯ç§»æ¤ï¼‰         |
| **ç”Ÿå‘½å‘¨æœŸ** | ä»»åŠ¡åœæ­¢åé»˜è®¤åˆ é™¤                | æ˜¾å¼åˆ›å»ºå’Œåˆ é™¤               |
| **æ€§èƒ½å½±å“** | ä½å¼€é”€ï¼ˆå¼‚æ­¥+å¢é‡ï¼‰               | é«˜å¼€é”€ï¼ˆå…¨é‡åŒæ­¥ï¼‰           |

#### 3.è¯»å–æ•°æ®æº

**ä½œç”¨**ï¼šè¿æ¥å¤–éƒ¨ç³»ç»Ÿï¼ˆå¦‚ Kafkaã€æ–‡ä»¶ã€Socketï¼‰ï¼Œç”Ÿæˆåˆå§‹`DataStream`ã€‚

**å†…ç½®ç»„ä»¶:**

| **Source ç±»å‹**            | **åŠŸèƒ½æè¿°**                 |
| -------------------------- | ---------------------------- |
| `KafkaSource`              | è¯»å– Kafka æ¶ˆæ¯              |
| `FileSource`               | è¯»å–æ–‡ä»¶ï¼ˆCSVã€JSON ç­‰ï¼‰     |
| `SocketTextStreamFunction` | ä» Socket æ¥æ”¶æ•°æ®           |
| `RichSourceFunction`       | è‡ªå®šä¹‰æ•°æ®æºï¼ˆå¦‚æ•°æ®åº“è½®è¯¢ï¼‰ |

**ç¤ºä¾‹:**

```python
from pyflink.datastream.connectors import KafkaSource

source = KafkaSource.builder() \
    .set_bootstrap_servers("localhost:9092") \
    .set_topics("input_topic") \
    .set_group_id("my_group") \
    .build()

stream = env.from_source(source, watermark_strategy, "Kafka Source")
```

#### 4.æ•°æ®æ¸…æ´—è½¬æ¢

**ä½œç”¨**ï¼šå¯¹æ•°æ®æµè¿›è¡Œè½¬æ¢ã€è¿‡æ»¤ã€èšåˆç­‰å¤„ç†ã€‚

**å…³é”®æ“ä½œç¬¦**ï¼š

| æ“ä½œç±»å‹         | åŠŸèƒ½æè¿°                                     | ç¤ºä¾‹                                                         |
| ---------------- | -------------------------------------------- | ------------------------------------------------------------ |
| `map`            | ä¸€å¯¹ä¸€è½¬æ¢ï¼ˆå¦‚å­—ç¬¦ä¸²è½¬æ•´æ•°ï¼‰ã€‚               | `stream.map(lambda x: x * 2)`                                |
| `filter`         | è¿‡æ»¤æ»¡è¶³æ¡ä»¶çš„å…ƒç´ ã€‚                         | `stream.filter(lambda x: x > 10)`                            |
| `keyBy`          | æŒ‰æŒ‡å®šå­—æ®µåˆ†ç»„ï¼ˆç±»ä¼¼ SQL çš„ GROUP BYï¼‰ã€‚     | `stream.keyBy(lambda x: x.user_id)`                          |
| `reduce`         | åˆ†ç»„å†…èšåˆï¼ˆå¦‚æ±‚å’Œã€æœ€å¤§å€¼ï¼‰ã€‚               | `stream.keyBy(...).reduce(sum)`                              |
| `window`         | å®šä¹‰æ—¶é—´æˆ–è®¡æ•°çª—å£ï¼ˆå¦‚æ»šåŠ¨çª—å£ã€æ»‘åŠ¨çª—å£ï¼‰ã€‚ | `stream.window(TumblingEventTimeWindows.of(Time.seconds(5)))` |
| `join`/`connect` | è¿æ¥ä¸¤ä¸ªæµï¼ˆåŸºäºæ—¶é—´æˆ–æ¡ä»¶ï¼‰ã€‚               | `stream.join(other_stream).where(...).equalTo(...)`          |
| `flatMap`        | ä¸€å¯¹å¤šè½¬æ¢ï¼ˆå¦‚åˆ†è¯ï¼‰ã€‚                       | `stream.flatMap(lambda x: x.split())`                        |

#### 5. æ•°æ®æ±‡(Sink)

**ä½œç”¨**ï¼šå°†å¤„ç†ç»“æœå†™å…¥å¤–éƒ¨ç³»ç»Ÿã€‚

**å†…ç½®ç»„ä»¶**ï¼š

- `KafkaSink`ï¼šå†™å…¥ Kafka ä¸»é¢˜ã€‚
- `PrintSinkFunction`ï¼šæ‰“å°åˆ°æ ‡å‡†è¾“å‡ºã€‚
- `FileSink`ï¼šå†™å…¥æ–‡ä»¶ï¼ˆCSVã€JSON ç­‰ï¼‰ã€‚
- `JDBCSink`ï¼šå†™å…¥å…³ç³»å‹æ•°æ®åº“ã€‚

#### 6. çŠ¶æ€ç®¡ç†(State)

**ä½œç”¨**ï¼šåœ¨ç®—å­ä¸­ä¿å­˜å’Œè®¿é—®å†å²æ•°æ®ï¼ˆå¦‚ç´¯åŠ å™¨ã€ç¼“å­˜ï¼‰ã€‚

**çŠ¶æ€ç±»å‹**ï¼š

- **Keyed State**ï¼šä¸ç‰¹å®š key å…³è”ï¼ˆéœ€å…ˆ`keyBy`ï¼‰ï¼Œå¦‚`ValueState`ã€`ListState`ã€‚
- **Operator State**ï¼šä¸ç®—å­å®ä¾‹å…³è”ï¼ˆå¦‚ Kafka è¿æ¥å™¨çš„åç§»é‡ï¼‰ã€‚

çŠ¶æ€åç«¯ï¼ˆStateBackendï¼‰ï¼š

- `MemoryStateBackend`ï¼šå†…å­˜å­˜å‚¨ï¼ˆæµ‹è¯•ç”¨ï¼‰ã€‚
- `FsStateBackend`ï¼šæ–‡ä»¶ç³»ç»Ÿå­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰ã€‚
- `RocksDBStateBackend`ï¼šæœ¬åœ° RocksDB å­˜å‚¨ï¼ˆå¤§çŠ¶æ€ï¼‰ã€‚

#### 7.æ—¶é—´ä¸çª—å£(Time & Window)

**æ—¶é—´è¯­ä¹‰**ï¼š

- **äº‹ä»¶æ—¶é—´ï¼ˆEvent Timeï¼‰**ï¼šæ•°æ®ç”Ÿæˆçš„æ—¶é—´ï¼ˆéœ€å®šä¹‰ Watermark å¤„ç†ä¹±åºï¼‰ã€‚
- **å¤„ç†æ—¶é—´ï¼ˆProcessing Timeï¼‰**ï¼šFlink å¤„ç†æ•°æ®çš„ç³»ç»Ÿæ—¶é—´ã€‚

**çª—å£ç±»å‹**ï¼š

| çª—å£ç±»å‹             | åŠŸèƒ½æè¿°                                           |
| -------------------- | -------------------------------------------------- |
| æ»šåŠ¨çª—å£ï¼ˆTumblingï¼‰ | å›ºå®šå¤§å°ï¼Œä¸é‡å ï¼ˆå¦‚æ¯ 5 åˆ†é’Ÿä¸€ä¸ªçª—å£ï¼‰ã€‚          |
| æ»‘åŠ¨çª—å£ï¼ˆSlidingï¼‰  | å¯é‡å ï¼ˆå¦‚æ¯ 5 ç§’æ»‘åŠ¨ä¸€æ¬¡ï¼Œçª—å£å¤§å° 10 ç§’ï¼‰ã€‚      |
| ä¼šè¯çª—å£ï¼ˆSessionï¼‰  | åŸºäºä¸æ´»è·ƒé—´éš™ï¼ˆå¦‚ç”¨æˆ· 30 åˆ†é’Ÿæ— æ“ä½œåˆ™çª—å£ç»“æŸï¼‰ã€‚ |
| å…¨å±€çª—å£ï¼ˆGlobalï¼‰   | æ‰€æœ‰ç›¸åŒ key çš„å…ƒç´ åœ¨ä¸€ä¸ªçª—å£ï¼Œéœ€è‡ªå®šä¹‰è§¦å‘å™¨ã€‚    |

#### 8.è§¦å‘å™¨(Trigger)

**ä½œç”¨**ï¼šå®šä¹‰çª—å£ä½•æ—¶è§¦å‘è®¡ç®—ï¼ˆå¦‚æ—¶é—´åˆ°è¾¾ã€å…ƒç´ æ•°é‡ã€è‡ªå®šä¹‰æ¡ä»¶ï¼‰ã€‚

**å†…ç½®è§¦å‘å™¨**ï¼š

- `ProcessingTimeTrigger`ï¼šåŸºäºå¤„ç†æ—¶é—´ã€‚
- `EventTimeTrigger`ï¼šåŸºäºäº‹ä»¶æ—¶é—´å’Œ Watermarkã€‚
- `CountTrigger`ï¼šåŸºäºå…ƒç´ æ•°é‡ã€‚

#### 9.æ°´å°(Watermark)

**ä½œç”¨**ï¼šå¤„ç†äº‹ä»¶æ—¶é—´ä¸­çš„ä¹±åºæ•°æ®ï¼ŒæŒ‡ç¤º â€œæ—¶é—´è¿›åº¦â€ã€‚

**ç”Ÿæˆæ–¹å¼**ï¼š

- å‘¨æœŸæ€§ç”Ÿæˆï¼šæŒ‰å›ºå®šæ—¶é—´é—´éš”ï¼ˆå¦‚æ¯ç§’ï¼‰ç”Ÿæˆ Watermarkã€‚
- æ ‡ç‚¹ç”Ÿæˆï¼šåŸºäºç‰¹å®šäº‹ä»¶ï¼ˆå¦‚æ¶ˆæ¯ä¸­åŒ…å«æ—¶é—´æˆ³ï¼‰ç”Ÿæˆã€‚

### 4.3Table APIä¸SQL

#### Table API & SQL çš„æ ¸å¿ƒä½œç”¨

**æ‰¹æµç»Ÿä¸€**: åŒä¸€å¥—è¯­æ³•æ—¢å¯å¤„ç†é™æ€æ•°æ®ï¼ˆæ‰¹æ¨¡å¼ï¼‰ï¼Œä¹Ÿå¯å¤„ç†å®æ—¶æ•°æ®æµï¼ˆæµæ¨¡å¼ï¼‰ï¼Œåº•å±‚è‡ªåŠ¨ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ã€‚

**å£°æ˜å¼ç¼–ç¨‹**: ç”¨æˆ·åªéœ€æŒ‡å®šâ€œåšä»€ä¹ˆâ€ï¼ˆå¦‚ `SELECT`ã€`JOIN`ï¼‰ï¼Œæ— éœ€å…³å¿ƒâ€œå¦‚ä½•åšâ€ï¼ˆå¦‚ç®—å­é“¾ã€çŠ¶æ€ç®¡ç†ï¼‰ã€‚

**ä¸ Flink ç”Ÿæ€æ— ç¼é›†æˆ**: ä¸ DataStream/DataSet API äº’ç›¸è½¬æ¢ï¼Œæ”¯æŒ UDFï¼ˆç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼‰ã€‚

**é«˜æ€§èƒ½ä¼˜åŒ–**: åŸºäº Apache Calcite ä¼˜åŒ–å™¨ç”Ÿæˆé«˜æ•ˆæ‰§è¡Œè®¡åˆ’ï¼Œè‡ªåŠ¨é€‰æ‹© join ç®—æ³•ã€è°“è¯ä¸‹æ¨ç­‰ã€‚

#### **1. è¡¨ç¯å¢ƒ(TableEnvironment)**

**ä½œç”¨**ï¼šç¨‹åºå…¥å£ï¼Œç®¡ç†è¡¨æ³¨å†Œã€æŸ¥è¯¢æ‰§è¡Œå’Œç¯å¢ƒé…ç½®ã€‚

ç±»å‹ï¼š

- `StreamTableEnvironment`ï¼šæµå¤„ç†ç¯å¢ƒï¼ˆä¸ DataStream API é›†æˆï¼‰ã€‚
- `BatchTableEnvironment`ï¼šæ‰¹å¤„ç†ç¯å¢ƒï¼ˆä¸ DataSet API é›†æˆï¼‰ã€‚

ç¤ºä¾‹ï¼š 

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment, EnvironmentSettings

# åˆ›å»ºæµå¤„ç†ç¯å¢ƒ
env = StreamExecutionEnvironment.get_execution_environment()
settings = EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()
t_env = StreamTableEnvironment.create(env, environment_settings=settings)
```

#### **2. è¡¨(Table)**

**ä½œç”¨**ï¼šè¡¨ç¤ºç»“æ„åŒ–æ•°æ®ï¼ˆç±»ä¼¼æ•°æ®åº“è¡¨ï¼‰ï¼Œå¯ä»å¤–éƒ¨æºæˆ–è®¡ç®—ç”Ÿæˆã€‚

æ¥æºï¼š

- æ³¨å†Œå¤–éƒ¨è¡¨ï¼ˆå¦‚ Kafkaã€MySQLï¼‰ã€‚
- ä» DataStream/DataSet è½¬æ¢ã€‚
- æ‰§è¡Œ Table API/SQL æŸ¥è¯¢çš„ç»“æœã€‚

ç¤ºä¾‹ï¼š 

```python
# æ³¨å†Œ Kafka è¡¨
t_env.execute_sql("""
CREATE TABLE source_kafka (
    user_id STRING,
    item_id STRING,
    behavior STRING,
    ts TIMESTAMP(3)
) WITH (
    'connector' = 'kafka',
    'topic' = 'user_behavior',
    'properties.bootstrap.servers' = 'localhost:9092',
    'format' = 'json'
)
""")

# æŸ¥è¯¢è¡¨
table = t_env.from_path("source_kafka")
```

#### **3. è§†å›¾(View)**

**ä½œç”¨**ï¼šä¸´æ—¶è¡¨ï¼ŒåŸºäºç°æœ‰è¡¨çš„æŸ¥è¯¢ç»“æœï¼Œä¸å­˜å‚¨æ•°æ®ã€‚

ç¤ºä¾‹ï¼š

```python
# åˆ›å»ºè§†å›¾
t_env.create_temporary_view(
    "user_view",
    t_env.from_path("source_kafka").filter("behavior = 'click'")
)
```

#### **4. è¿æ¥å™¨(Connector)**

**ä½œç”¨**ï¼šè¿æ¥å¤–éƒ¨ç³»ç»Ÿï¼ˆå¦‚æ•°æ®åº“ã€æ¶ˆæ¯é˜Ÿåˆ—ï¼‰ã€‚

å†…ç½®è¿æ¥å™¨ï¼š

- Kafkaã€RabbitMQï¼šæ¶ˆæ¯é˜Ÿåˆ—ã€‚
- JDBCã€MySQLã€PostgreSQLï¼šå…³ç³»å‹æ•°æ®åº“ã€‚
- Elasticsearchã€HBaseï¼šNoSQL æ•°æ®åº“ã€‚
- FileSystemï¼šæ–‡ä»¶ç³»ç»Ÿï¼ˆCSVã€JSONã€Parquetï¼‰ã€‚

ç¤ºä¾‹

```python
# åˆ›å»º MySQL è¾“å‡ºè¡¨
t_env.execute_sql("""
CREATE TABLE sink_mysql (
    behavior STRING,
    cnt BIGINT
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://localhost:3306/flink',
    'table-name' = 'behavior_count',
    'username' = 'root',
    'password' = 'password'
)
""")
```

#### **5. å‡½æ•°ï¼ˆFunctionï¼‰**

**ä½œç”¨**ï¼šæ‰©å±• SQL åŠŸèƒ½ï¼Œæ”¯æŒè‡ªå®šä¹‰é€»è¾‘ã€‚

ç±»å‹ï¼š

- **æ ‡é‡å‡½æ•°ï¼ˆScalar Functionï¼‰**ï¼šè¾“å…¥ä¸€è¡Œè¿”å›ä¸€ä¸ªå€¼ï¼ˆå¦‚ UDFï¼‰ã€‚
- **è¡¨å‡½æ•°ï¼ˆTable Functionï¼‰**ï¼šè¾“å…¥ä¸€è¡Œè¿”å›å¤šè¡Œï¼ˆå¦‚ UDTFï¼‰ã€‚
- **èšåˆå‡½æ•°ï¼ˆAggregate Functionï¼‰**ï¼šè¾“å…¥å¤šè¡Œè¿”å›ä¸€ä¸ªå€¼ï¼ˆå¦‚ SUMï¼‰ã€‚
- **è¡¨èšåˆå‡½æ•°ï¼ˆTable Aggregate Functionï¼‰**ï¼šè¾“å…¥å¤šè¡Œè¿”å›å¤šè¡Œï¼ˆå¦‚ TopNï¼‰ã€‚

ç¤ºä¾‹

```python
from pyflink.table.udf import udf

# æ³¨å†Œ UDF
@udf(result_type=DataTypes.STRING())
def upper(s):
    return s.upper() if s else None

t_env.create_temporary_function("upper", upper)

# åœ¨ SQL ä¸­ä½¿ç”¨
result = t_env.sql_query("SELECT upper(name) FROM users")
```

















### 4.4DataSet API

Flink ç¤¾åŒºå·²æ˜ç¡®å°† **Table API/SQL** ä½œä¸ºæ‰¹æµç»Ÿä¸€çš„ä¸»è¦æ¥å£ï¼ŒDataSet API åœ¨ Flink 1.12 åè¢«æ ‡è®°ä¸º**è¿‡æ—¶**ï¼ˆdeprecatedï¼‰ï¼Œæœ€ç»ˆå¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬ç§»é™¤ã€‚

**æ–°é¡¹ç›®é¿å…ä½¿ç”¨ DataSet API**ï¼Œä¼˜å…ˆé€‰æ‹© **Table API/SQL** æˆ– **DataStream APIï¼ˆæ‰¹æ¨¡å¼ï¼‰**ã€‚

æ ¸å¿ƒåŠŸèƒ½:

- **æ‰¹å¤„ç†è®¡ç®—**: é’ˆå¯¹é™æ€æ•°æ®é›†ï¼ˆå¦‚ HDFS æ–‡ä»¶ã€æ•°æ®åº“è¡¨ï¼‰è¿›è¡Œåˆ†å¸ƒå¼å¤„ç†ï¼Œé€‚åˆç¦»çº¿åˆ†æåœºæ™¯ã€‚
- **ä¸°å¯Œçš„ç®—å­æ”¯æŒ**: æä¾›ç±»ä¼¼ MapReduce ä½†æ›´é«˜å±‚çš„æ“ä½œï¼ˆå¦‚ `join`ã€`groupBy`ã€`aggregate`ï¼‰ï¼Œç®€åŒ–å¼€å‘ã€‚
- **å†…å­˜ä¼˜åŒ–æ‰§è¡Œ**: é€šè¿‡ pipelined æ‰§è¡Œæ¨¡å¼å‡å°‘ç£ç›˜ I/Oï¼Œæå‡æ€§èƒ½ã€‚
- **å®¹é”™æœºåˆ¶**ï¼š åŸºäºé‡è¯•ï¼ˆRecoveryï¼‰çš„å®¹é”™ï¼Œè€Œéæµå¤„ç†ä¸­çš„ Checkpointingã€‚

#### **1. æ•°æ®æºï¼ˆDataSourceï¼‰**

**åŠŸèƒ½**ï¼šä»å¤–éƒ¨ç³»ç»Ÿè¯»å–æ•°æ®ï¼Œè½¬æ¢ä¸º Flink æ•°æ®é›†ã€‚

**å¸¸è§å®ç°**ï¼š

- æ–‡ä»¶ç³»ç»Ÿï¼š`readTextFile()`ã€`readCsvFile()`
- æ•°æ®åº“ï¼šé€šè¿‡ `JDBCInputFormat` è¯»å–å…³ç³»å‹æ•°æ®
- Hadoop å…¼å®¹æºï¼š`HadoopInputFormat`

#### **2. æ•°æ®è½¬æ¢ï¼ˆTransformationsï¼‰**

**åŠŸèƒ½**ï¼šå¯¹æ•°æ®é›†è¿›è¡Œè½¬æ¢æ“ä½œï¼Œç”Ÿæˆæ–°çš„æ•°æ®é›†ã€‚

**å…³é”®æ“ä½œ**ï¼š

- **Map/FlatMap/Filter**ï¼šé€æ¡å¤„ç†æ•°æ®ã€‚
- **Reduce/GroupReduce**ï¼šæŒ‰é”®åˆ†ç»„åèšåˆï¼ˆå¦‚æ±‚å’Œã€æ±‚å¹³å‡å€¼ï¼‰ã€‚
- **Join/CoGroup**ï¼šå¤šè¡¨å…³è”ï¼ˆç±»ä¼¼ SQL çš„ JOINï¼‰ã€‚
- **Distinct**ï¼šå»é‡ã€‚
- **Union**ï¼šåˆå¹¶å¤šä¸ªæ•°æ®é›†ã€‚

#### **3. æ•°æ®æ±‡ï¼ˆDataSinkï¼‰**

- **åŠŸèƒ½**ï¼šå°†å¤„ç†ç»“æœè¾“å‡ºåˆ°å¤–éƒ¨ç³»ç»Ÿã€‚
- **å¸¸è§å®ç°**ï¼š
  - æ–‡ä»¶ç³»ç»Ÿï¼š`writeAsText()`ã€`writeAsCsv()`
  - æ•°æ®åº“ï¼š`JDBCOutputFormat`
  - Hadoop å…¼å®¹è¾“å‡ºï¼š`HadoopOutputFormat`

#### **4. è¿­ä»£è®¡ç®—ï¼ˆIterationsï¼‰**

- **åŠŸèƒ½**ï¼šæ”¯æŒæ‰¹é‡è¿­ä»£ï¼ˆå¦‚æœºå™¨å­¦ä¹ ä¸­çš„æ¢¯åº¦ä¸‹é™ï¼‰ã€‚
- **ç±»å‹**ï¼š
  - **Bulk Iteration**ï¼šå›ºå®šæ¬¡æ•°çš„å…¨é‡è¿­ä»£ã€‚
  - **Delta Iteration**ï¼šä»…å¤„ç†å˜åŒ–çš„å¢é‡è¿­ä»£ã€‚

#### **5. åˆ†åŒºç­–ç•¥ï¼ˆPartitioningï¼‰**

- **åŠŸèƒ½**ï¼šæ§åˆ¶æ•°æ®åˆ†å¸ƒï¼Œä¼˜åŒ–æ€§èƒ½ã€‚
- **ç­–ç•¥**ï¼š
  - **Hash Partitioning**ï¼šæŒ‰é”®å“ˆå¸Œåˆ†å¸ƒã€‚
  - **Range Partitioning**ï¼šæŒ‰èŒƒå›´åˆ†å¸ƒã€‚
  - **Rebalance**ï¼šå¼ºåˆ¶å‡åŒ€åˆ†å¸ƒã€‚







## Flink Practice

### 1.Amazonå®æ—¶æ•°æ®ETL

å®ç°:  Amazon APIç¨‹åºæ”¶é›†æ•°æ® â†’ Kafka (zookeeper) â†’ Flink â†’ MySQL

```mermaid
flowchart TB
    subgraph Docker_Network["Docker Networkï¼ˆå­¦ä¹ ç¯å¢ƒï¼‰"]
        A[API Producer] -->|Push JSON events| B[Kafka Broker]
        B -->|Poll messages| C[Flink]
        C -->|Upsert results| D[MySQL]
        
        Z[ZooKeeper] -.->|Manage metadata| B
    end

    style Z stroke:#999,stroke-width:2px
    style C fill:#f9f,stroke:#333
```

| ç»„ä»¶               | èŒè´£                                                        | æ˜¯å¦å¿…é¡»                     |
| ------------------ | ----------------------------------------------------------- | ---------------------------- |
| **Amazon APIç¨‹åº** | ç”ŸæˆåŸå§‹æ•°æ®ï¼ˆå¦‚å•†å“ç‚¹å‡»æµã€è®¢å•æ—¥å¿—ç­‰ï¼‰ï¼Œæ¨é€è‡³Kafka       | æ•°æ®æºï¼Œå¯æ›¿æ¢ä¸ºå…¶ä»–ç”Ÿäº§è€…   |
| **Kafka**          | 1. ç¼“å†²é«˜ååæ•°æ®æµ 2. è§£è€¦ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€… 3. æŒä¹…åŒ–ä¸´æ—¶æ•°æ® | å¿…é¡»ï¼ˆæµå¤„ç†æ ¸å¿ƒä¸­é—´ä»¶ï¼‰     |
| **ZooKeeper**      | 1. ç®¡ç†Kafka Brokeræ³¨å†Œä¸å¿ƒè·³ 2. ç»´æŠ¤Topic/Partitionå…ƒæ•°æ®  | Kafkaä¼ ç»Ÿæ¨¡å¼å¿…é¡»ï¼ŒKRaftå¯é€‰ |
| **Flink**          | 1. å®æ—¶æ¶ˆè´¹Kafkaæ•°æ® 2. æ‰§è¡ŒETL/èšåˆè®¡ç®— 3. å°†ç»“æœå†™å…¥MySQL | å¿…é¡»ï¼ˆæµå¤„ç†å¼•æ“ï¼‰           |
| **MySQL**          | å­˜å‚¨æ¸…æ´—åçš„ç»“æ„åŒ–æ•°æ®ï¼Œä¾›ä¸‹æ¸¸æŸ¥è¯¢æˆ–åˆ†æ                    | å¯æ›¿æ¢ä¸ºå…¶ä»–æ•°æ®åº“           |

Flinkç»„ä»¶ç»†åŒ–è¯´æ˜

```mermaid
flowchart TB
    subgraph Flink_Cluster["Flink Processing (Detailed)"]
        direction TB
        
        %% ===== Input Phase =====
        B[(Kafka Broker)] -->|Poll Events| C1[TaskManager Slot]
        C1 -->|"FlinkKafkaConsumer\n(Source API)"| C2[Source Operator]
        
        %% ===== Processing Phase =====
        C2 -->|DataStream| C3[Map Operator]
        C3 -->|"DataStream API\n(filter/map)"| C4[KeyedStream]
        C4 -->|"Window API\n(tumblingWindow)"| C5[Aggregate Operator]
        
        %% ===== Output Phase =====
        C5 -->|"JdbcSink\n(Sink API)"| C6[Sink Operator]
        C6 -->|UPSERT| D[(MySQL)]
        
        %% ===== Coordination =====
        J[JobManager] -->|"Deploy JobGraph"| C1
        J -->|"Assign Slots"| C3
        J -->|"Checkpoint"| C5
    end

    %% ===== Style Definitions =====
    style B fill:#f9f,stroke:#333
    style C1 fill:#cff,stroke:#333
    style C3 fill:#fcf,stroke:#333
    style C5 fill:#9f9,stroke:#333
    style J fill:#f96,stroke:#333
    style D fill:#f9f,stroke:#333

```

| æµç¨‹å›¾ç»„ä»¶             | Flink å®ä½“            | API/ç±»                        | æ‰§è¡Œä½ç½®         | èŒè´£åŠŸèƒ½                                                     |
| ---------------------- | --------------------- | ----------------------------- | ---------------- | ------------------------------------------------------------ |
| **Source Operator**    | Kafka Consumer Task   | `FlinkKafkaConsumer`          | TaskManager Slot | ä»KafkaæŒç»­æ‹‰å–æ•°æ®ï¼Œååºåˆ—åŒ–ä¸ºFlinkå†…éƒ¨æ•°æ®æ ¼å¼ï¼ˆå¦‚POJO/Rowï¼‰ï¼Œæ”¯æŒç²¾ç¡®ä¸€æ¬¡æ¶ˆè´¹ |
| **Map Operator**       | Stream Transformation | `DataStream.map()`/`filter()` | TaskManager Slot | å®ç°å­—æ®µæå–ã€æ•°æ®è¿‡æ»¤ã€æ ¼å¼è½¬æ¢ç­‰è½»é‡çº§æ¸…æ´—é€»è¾‘             |
| **KeyBy Operator**     | Partitioning Operator | `keyBy()`                     | TaskManager Slot | æŒ‰æŒ‡å®šKeyå“ˆå¸Œåˆ†åŒºï¼Œç¡®ä¿ç›¸åŒKeyçš„æ•°æ®å‘é€åˆ°åŒä¸€ä¸ªä¸‹æ¸¸å®ä¾‹     |
| **Window Operator**    | Windowed Computation  | `TumblingEventTimeWindows`    | TaskManager Slot | å®šä¹‰çª—å£èŒƒå›´ï¼ˆå¦‚5åˆ†é’Ÿæ»šåŠ¨çª—å£ï¼‰ï¼Œè§¦å‘çª—å£è®¡ç®—æ—¶æœº            |
| **Aggregate Operator** | Stateful Computation  | `aggregate()`/`reduce()`      | TaskManager Slot | æ‰§è¡ŒSUM/COUNT/MAXç­‰èšåˆè®¡ç®—ï¼Œç»“æœä¿å­˜åœ¨OperatorStateä¸­       |
| **Sink Operator**      | JDBC Writer           | `JdbcSink.sink()`             | TaskManager Slot | æ‰¹é‡å†™å…¥MySQLï¼Œæ”¯æŒäº‹åŠ¡æäº¤ï¼ˆExactly-Onceè¯­ä¹‰ï¼‰              |
| **JobManager**         | Flink Master          | `JobGraph`                    | ç‹¬ç«‹è¿›ç¨‹         | åè°ƒä»»åŠ¡è°ƒåº¦ã€æ•…éšœæ¢å¤ï¼ˆCheckpointï¼‰ã€èµ„æºåˆ†é…ï¼ˆSlotsï¼‰      |



### 2.Flinkè¶…ç®€å•æµ‹è¯•ä¾‹å­

```mermaid
flowchart TB
    subgraph Flinkæœ¬åœ°è¿·ä½ é›†ç¾¤
        JM[JobManager] -->|æäº¤ä½œä¸š| TM[TaskManager]
    end

    subgraph æ•°æ®å¤„ç†æµæ°´çº¿
        FS[FileSource\nè¯»å–JSONæ–‡ä»¶] -->|åŸå§‹JSONå­—ç¬¦ä¸²| PARSER[JSONè§£æå™¨\nè½¬POJO/Row]
        PARSER -->|ç»“æ„åŒ–æ•°æ®| CLEAN[æ¸…æ´—ç®—å­é“¾\nè¿‡æ»¤/è½¬æ¢/æ‰å¹³åŒ–]
        CLEAN -->|æ¸…æ´—åæ•°æ®| SINK[FileSink\nå†™å…¥æœ¬åœ°æ–‡ä»¶]
    end

    LOCAL[(æœ¬åœ°JSONæ–‡ä»¶)] --> FS
    SINK --> OUTPUT[(è¾“å‡ºæ–‡ä»¶)]

    style LOCAL fill:#9cf,stroke:#333
    style OUTPUT fill:#9cf,stroke:#333
    style JM fill:#f96,stroke:#333
    style TM fill:#c9f,stroke:#333

```

```mermaid
flowchart LR
    P[Pythonå‘é€ç«¯] --Socket/HTTP--> F[Flink Source]
    F -->|åŸå§‹JSON| C[æ¸…æ´—ç®—å­]
    C -->|ç»“æ„åŒ–æ•°æ®| S[FileSink]
    S --> O[(æœ¬åœ°æ–‡ä»¶)]
    
    style P fill:#9cf,stroke:#333
    style F fill:#c9f,stroke:#333
    style S fill:#f96,stroke:#333

```

